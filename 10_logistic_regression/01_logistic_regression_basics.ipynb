{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a9216c9-d5b7-43d6-9fd7-f05318166c67",
   "metadata": {},
   "source": [
    "<font size=\"6\"><b>GENERALIZED LINEAR MODELS AND LOGISTIC REGRESSION: BASICS</b></font>\n",
    "\n",
    "<font size=\"5\"><b>Serhat Çevikel</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc99bd7-3fc7-494a-89d5-d2883abf7ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(data.table)\n",
    "library(tidyverse)\n",
    "library(plotly)\n",
    "library(broom) # for extracting coefficients\n",
    "library(caret) # for confusion matrix\n",
    "library(pROC) # for roc and auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b9d99-08ff-4095-9037-3fca9b983911",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.matrix.max.rows=20, repr.matrix.max.cols=15) # for limiting the number of top and bottom rows of tables printed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b909b17a-b0bf-4910-a5e1-c61f929b0a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath <- \"~/databa\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6eb1e1-3531-4c63-911b-17ca9b759b07",
   "metadata": {},
   "source": [
    "![xkcd](../imagesba/odds_ratio.png)\n",
    "\n",
    "(https://xkcd.com/2599/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b74df60-4923-4a32-b4f4-5bf23c13a42f",
   "metadata": {},
   "source": [
    "# Simulating and Modeling Bernoulli Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1fd778-cd17-4684-aa33-59e7b211ca6e",
   "metadata": {},
   "source": [
    "We know that the expected value of a random variable is its mean:\n",
    "\n",
    "\n",
    "For discrete variables with finite number of possible values:\n",
    "\n",
    "${\\displaystyle \\operatorname {E} [X]=\\sum _{i=1}^{n}x_{i}\\,p_{i},}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c6c192-4661-4cb1-8817-d598f1c92f05",
   "metadata": {},
   "source": [
    "And for continuous variables with infinite number of possible values:\n",
    "\n",
    "${\\displaystyle \\operatorname {E} [X]=\\int _{-\\infty }^{\\infty }xf(x)\\,dx.}$\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Expected_value#Random_variables_with_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac4fec6-019c-4799-844a-b406e8929825",
   "metadata": {},
   "source": [
    "A linear regression model basically models the **conditional mean** ($\\boldsymbol \\mu$) of a response variable given the values of a set of predictors:\n",
    "\n",
    "${\\displaystyle \\operatorname {E} (\\mathbf {Y} \\mid \\mathbf {X} )={\\boldsymbol {\\mu }}=\\mathbf {X} {\\boldsymbol {\\beta }}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d9181-390a-488f-bd47-461e6cdc91a9",
   "metadata": {},
   "source": [
    "While this formulation is for the conditional mean of the response variable, the model also includes an error term to account for the variability around this conditional mean:\n",
    "\n",
    "${\\displaystyle \\mathbf {y} =\\mathbf {X} {\\boldsymbol {\\beta }}+{\\boldsymbol {\\varepsilon }}}$\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Linear_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc0240-6813-468d-9aa3-090157962129",
   "metadata": {},
   "source": [
    "And one of the assumptions of ordinary least squares estimator in linear regression is the assumption of equal variance or homoscedasticity:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96fd50-e6c6-456b-981b-5a4ae972b314",
   "metadata": {},
   "source": [
    "$\\operatorname E( \\epsilon_i^2 | X) = \\sigma^2$\n",
    "\n",
    "So the variance of the errors conditional on the values of the predictors is the same for all predictor values.\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Ordinary_least_squares#Assumptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f24f5-1a47-463d-ac72-772c94de0bec",
   "metadata": {},
   "source": [
    "In linear regression models with OLS, we worked with predictors and response variables with unbounded values between $-\\infty$ and $\\infty$ and in which variance is a parameter separate from the expected value and therefore set separately. This was the case with normally distributed variables. Hence:\n",
    "\n",
    "- All fitted values in the continuum can be meaningful since response can take any value\n",
    "- And with some transformations equal variance assumption could be met to a certain degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb63072-2a82-4c15-a6c4-68cb5e902379",
   "metadata": {},
   "source": [
    "We know that, in normal distribution, variance is a free parameter that we can set for the distribution:\n",
    "\n",
    "${\\displaystyle {\\mathcal {N}}(\\mu ,\\sigma ^{2})}$\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Normal_distribution)\n",
    "\n",
    "The mean sets the location of the distribution while variance sets the scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14cab7e-42e9-4e07-b036-c05efde6f553",
   "metadata": {},
   "source": [
    "However, we know that in some distribution types, the distribution is defined by only location and shape parameters and variance is bound to those values and not set separately.\n",
    "\n",
    "The examples are:\n",
    "\n",
    "- Binomial distribution: ${\\displaystyle \\operatorname {Var} (X)=npq=np(1-p)}$\n",
    "- Poisson distribution: ${\\displaystyle \\operatorname {Var} (X)=\\lambda}$\n",
    "- Gamma distribution: ${\\displaystyle \\operatorname {Var} (X)=\\alpha /\\lambda ^{2}}$\n",
    "\n",
    "Furthermore the variables drawn from these distributions are not supported on the whole continuum. Poisson and Gamma distributions have a support on non-negative values and Binomial distrubtion has a support between 0 and $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d43ed4-ae42-441f-994b-cae7a749e8c6",
   "metadata": {},
   "source": [
    "Let's say for example for different p values, the expected variance of Bernoulli distribution will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ce294-5050-48c3-8f3a-034baf19d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "px <- seq(0, 1, 1e-2)\n",
    "plot(px, px*(1-px), type = \"l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e85fb6-aca8-4a56-8b73-72a508b0c3ef",
   "metadata": {},
   "source": [
    "Now let's make a new simulation in which response variables can only take 0 and 1 values, from Bernoulli distribution which is a special case of Binomial distribution with $n=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d782bd-a556-41eb-b24a-bff762ee60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs <- 1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1ee47-ca01-4158-9240-2d4fe6295da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(5)\n",
    "xterm <- rnorm(nobs, 2, 1)\n",
    "#eterm <- normalize(rnorm(nobs, 0, 2))\n",
    "beta0 <- -4\n",
    "beta1 <- 2\n",
    "yterm_raw <- beta0 + beta1 * xterm\n",
    "data1 <- data.table(yterm_raw, xterm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea80ca-9bfb-4d05-9187-6e3c4ecfd16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d9b2e-d678-406c-9a00-ff473ccf1460",
   "metadata": {},
   "source": [
    "Note that we did not include an error term but the uncertainty around the expected mean of the response variable will be simulated in a different way:\n",
    "\n",
    "- First we will compress the raw y values such that they are always between 0 and 1\n",
    "- Each compresed value will be treated as the p of a Bernoulli trial and for each p, 0 and 1 values will be drawn from the respective Bernoulli distribution, like a coin toss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65286727-dbd7-4e8c-bfa7-25c6a24e785a",
   "metadata": {},
   "source": [
    "The function to compress values on the continuum into the (0, 1) range will be the logistic function. We will come to the importance and the origin of this function shortly. Let's see how values are mapped with the logistic function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d55c5a-20a5-4c0d-9a74-c9c3da04323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic <- function(x) 1/(1 + exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e380c6e-cb15-4500-9724-90a9cbab6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "yx <- seq(-10, 10, 1e-2)\n",
    "plot(yx, logistic(yx), type = \"l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0b858-cf41-4cc7-94f6-88a2ab2eb8b9",
   "metadata": {},
   "source": [
    "The S shape is known as sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360391a0-a78f-4d00-8b12-cc4203f39d3b",
   "metadata": {},
   "source": [
    "Now let's convert raw t values into probability values of Bernoulli trials and sample a single value for each p:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2830e73a-21aa-4d6f-8e1c-9e4cce353148",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[, px := logistic(yterm_raw)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ad2ac-3a00-4de6-8a9a-a4d33bca81a7",
   "metadata": {},
   "source": [
    "The distribution of the raw y values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac826b-26c1-42a5-88e6-d131d3fbdffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(data1$yterm_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be0a1f8-79d4-438f-b62c-045f30357f7f",
   "metadata": {},
   "source": [
    "And the distribution of transformed p values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f845f47-96e6-48c0-9b3d-4ec6df773d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(data1$px)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e9890-03c5-4003-a7d9-49e8ca9ac7a4",
   "metadata": {},
   "source": [
    "Now let's toss a single coin for each p value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dedd49-0a10-4473-82c2-68edb518d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(10)\n",
    "data1[, yterm := rbinom(.N, 1, px)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6163f64-9e2e-41ea-839d-3271bbf16ddc",
   "metadata": {},
   "source": [
    "Let's see the distribution of binomial responses across p-values, with some added jitter (random vertical noise) so that the dots are separated from each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f2c1c0-eff0-4fb5-889a-57071f832385",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(20)\n",
    "data1 %>%\n",
    "ggplot(aes(x = px, y = yterm)) +\n",
    "geom_jitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97071b1-530b-4d6a-9d64-3adeba07e208",
   "metadata": {},
   "source": [
    "Now let's see the variance of binomial responses across different p values. For this we create 20 bins of p values with a width of 5% each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8a6dd-4a4e-4cff-bc1f-d05a02f149fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[, pxc := cut(px, breaks = seq(0, 1, 0.05), labels = seq(0, 1 - 0.05, 0.05))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67517b4-6c82-4a03-b2f6-6d56b1a1fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 %>%\n",
    "group_by(pxc) %>%\n",
    "summarise(vary = var(yterm)) %>%\n",
    "ggplot(aes(x = pxc, y = vary, group = 1)) +\n",
    "geom_line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e2193-1160-4a24-a270-f08f84fb7b88",
   "metadata": {},
   "source": [
    "We see that variance is not the same across p values, in line with binomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853681d0-deaa-48d5-aa7d-116a9d6638b9",
   "metadata": {},
   "source": [
    "Now let's try to model the binomial responses with the raw x values as the sole predictor using ordinary least squares:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f597700-9ba0-4a83-9cee-0bec2587dc0f",
   "metadata": {},
   "source": [
    "## Modeling Bernoulli Responses with OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f987b9d-97af-4377-b3c5-21a58bd6b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ls1 <- lm(yterm ~ xterm, data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8022ab54-b186-42e4-b5fe-0fa683d72e06",
   "metadata": {},
   "source": [
    "By just looking at test diagnostics, we can say we have a significant model and coefficients and we could explain around 36% of the total variation in the binomial response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa0cadc-17d3-4105-8963-44ad1ffe907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model_ls1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c61bf-031d-4254-8e9b-a9757b9c8969",
   "metadata": {},
   "source": [
    "However the diagnostic plots says that, neither we have a constant variance or normally distributed residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaee46a-5962-4fb2-affc-f7967cee02bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(model_ls1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4abcf90-2ca3-4762-9925-babdfcea88d5",
   "metadata": {},
   "source": [
    "Now let's get the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718d3beb-93ae-46a6-8544-9eb63a40c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ls1 <- predict(model_ls1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d3802-cc63-4b6a-b402-df7a6c4ab338",
   "metadata": {},
   "source": [
    "And see their distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41469a91-3b57-4bd5-a44d-a33ab8abf09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(pred_ls1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077aaa55-d444-4a9c-b334-e005582ef903",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(pred_ls1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4504faa2-621d-44dc-806f-04dac587d452",
   "metadata": {},
   "source": [
    "For the binomial response of 0 and 1 values we could have predictions as low as -0.7 and as high as 1.57, not meaningful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0dc478-b0dc-45b2-a0bc-74b2373c3c9b",
   "metadata": {},
   "source": [
    "## Logit Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe595d-19d5-4262-8722-b80bdea30448",
   "metadata": {},
   "source": [
    "The predictions can be thought of as the p values, where each p value is the probability to get 1 in the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b590715-cf5c-4014-9669-84826707c6f5",
   "metadata": {},
   "source": [
    "Since the probability can only be between 0 and 1, while the linear model's prediction can have any values between $-\\infty$ and $\\infty$, let's make some transformations.\n",
    "\n",
    "First let's transform the probabilities to odds:\n",
    "\n",
    "${\\displaystyle {\\frac {p}{1-p}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83058807-cf07-4823-8633-f4dc64f06ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "odds <- function(x) x / (1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f4e54e-df56-4da5-bea6-136712014533",
   "metadata": {},
   "outputs": [],
   "source": [
    "yx <- seq(0.05, 0.95, 1e-2)\n",
    "plot(yx, odds(yx), type = \"l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77793015-fbcc-4f65-92fb-6407578d5c4e",
   "metadata": {},
   "source": [
    "Now the odd values can have any value between 0 and $\\infty$. We are much better. But still we cannot have negative values.\n",
    "\n",
    "Let's do a second transformation and get the natural logarithm of the odds values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636cc35-3e4b-4aca-8c58-292dcfee88b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "yx <- seq(0.05, 0.95, 1e-2)\n",
    "yxod <- odds(yx)\n",
    "plot(yxod, log(yxod), type = \"l\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2f287-7518-4fa6-aa01-81e743ba122a",
   "metadata": {},
   "source": [
    "Now we can have both negative and positive values in the whole continuum.\n",
    "\n",
    "If we combine the two steps we get the transformation of log odds:\n",
    "\n",
    "${\\displaystyle \\operatorname {logit} p=\\ln {\\frac {p}{1-p}}\\quad {\\text{for}}\\quad p\\in (0,1).}$\n",
    "\n",
    "This function is known as the **logit** function.\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4793a7-3816-4953-949f-ca960052ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit <- function(x) log(x / (1-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12fb65-9555-420c-b639-ec9381633f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "yx <- seq(0.01, 0.99, 1e-2)\n",
    "plot(yx, logit(yx), type = \"l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693827ab-0a57-44c0-ba24-9641f32e0dac",
   "metadata": {},
   "source": [
    "And what's the relationship between the logistic and logit functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82764705-b448-4e53-a3ab-283706baf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "yx <- seq(0.01, 0.99, 1e-2)\n",
    "plot(yx, logistic(logit(yx)), type = \"l\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d39990-7400-44ff-8f9a-6b2237bbac07",
   "metadata": {},
   "source": [
    "They are inverses of each other. So:\n",
    "\n",
    "- `logit` function takes the log odds of a probability value between 0 and 1 and maps into any value in the continuum:\n",
    "\n",
    "  ${\\displaystyle \\operatorname {logit} p=\\ln {\\frac {p}{1-p}}\\quad {\\text{for}}\\quad p\\in (0,1).}$\n",
    "\n",
    "- `logistic` function is the inverse of `logit`: takes a value in the continuum and maps into the (0,1) interval:\n",
    "\n",
    "${\\displaystyle f(x)={\\frac {1}{1+e^{-x}}}}$\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Logistic_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb44770-1b01-4416-bdd3-28b3b258ae8b",
   "metadata": {},
   "source": [
    "# Generalized Linear Models and Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741a1418-db5f-4b0d-acf9-0fcf1f569516",
   "metadata": {},
   "source": [
    "Now let's come to the definition of *generalized linear models*:\n",
    "\n",
    "> The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\n",
    "\n",
    "\n",
    "The GLM consists of three elements:\n",
    "\n",
    "- A particular distribution for modeling ${\\displaystyle Y}$ from among those which are considered exponential families of probability distributions,\n",
    "- A linear predictor ${\\displaystyle \\eta =X\\beta }$ and\n",
    "- A link function ${\\displaystyle g}$ such that ${\\displaystyle \\operatorname {E} (Y\\mid X)=\\mu =g^{-1}(\\eta )}$\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Generalized_linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b7db7f-1db2-4867-bcdc-a0f1ebc05a4f",
   "metadata": {},
   "source": [
    "In our simple example, \n",
    "\n",
    "- Y is supposed to be drawn from a Bernoulli distribution, which is a special case of Binomial distribution\n",
    "- The link function $g$ that maps the predicted responses $\\hat \\y$ in the $(0, 1)$ interval to the output of the linear predictor $\\eta$ in the $(\\infty, -\\infty)$ interval is `logit` function.\n",
    "- The inverse link function $g^-1$ that maps the output of the linear predictor $\\eta$ in the $(\\infty, -\\infty)$ interval onto the the predicted probability values in the $(0, 1)$ interval is `logistic` function.\n",
    "\n",
    "Hence our model will be called *logistic regression*.\n",
    "\n",
    "However we cannot use the OLS method to estimate the model parameters as we saw above. We should follow a different approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671929e9-0a4f-4445-90bf-0b3fbdd70ee5",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67a837-5562-45d0-9893-e956d4cc7857",
   "metadata": {},
   "source": [
    "Let's now calculate the likelihood values $\\displaystyle {\\mathcal {L}}$ of the binomial responses given the predicted probabilities made by estimated parameters.\n",
    "\n",
    "Note that for any 0 and 1 value, for a given p, the probability mass function of Bernoulli distribution is:\n",
    "\n",
    "${\\displaystyle f(k;p)={\\begin{cases}p&{\\text{if }}k=1,\\\\q=1-p&{\\text{if }}k=0.\\end{cases}}}$\n",
    "\n",
    "or\n",
    "\n",
    "${\\displaystyle f(k;p)=p^{k}(1-p)^{1-k}\\quad {\\text{for }}k\\in \\{0,1\\}}$\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Bernoulli_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca030ac-5960-49af-9931-17aeb20f0389",
   "metadata": {},
   "source": [
    "Since we don't know the true model and try to estimate the model parameters. Let's start with some arbitrary parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc9301-32af-4b17-86b5-6123fc3fdaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "b00 <- -8\n",
    "b10 <- 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4494ae6-5242-436c-b23a-9792f6bea218",
   "metadata": {},
   "source": [
    "Let's calculate the fitted p-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203d25b-3651-4741-9578-5113739ee43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yfit1 <- logistic(b00 + b10 * data1$xterm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae4940-e5bd-4e4d-abfe-1516f19584f2",
   "metadata": {},
   "source": [
    "And calculate the Binomial likelihood of the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef4799-e346-4613-ba30-582182607528",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh1 <- dbinom(data1$yterm, 1, yfit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c1a03-e672-4926-a345-f37c305e2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(yfit1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43412f-441d-4c01-99ce-a9790cdda2d2",
   "metadata": {},
   "source": [
    "That's basically, taking the fitted p values for observations that correspond to 1's, and taking $1 - p$ for observations that correspond to 0's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79941d88-b52b-4de4-ae9c-1638559dd09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh1b <- ifelse(data1$yterm, yfit1, 1 - yfit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b5ad2-8ba6-4a49-9c10-8cb879b07d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "identical(lh1, lh1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ebd796-534c-4cdc-afea-3ddae47c39e5",
   "metadata": {},
   "source": [
    "However in order to get the joint probability of individual likelihoods, we must take the product of these values, in order to calculate the likelihood of the estimated model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c3698-40e9-4763-a526-24ee3dd5987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lhm1 <- prod(lh1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317592c-299e-4a90-95bc-b8a62fa60c08",
   "metadata": {},
   "source": [
    "For 10,000 values the product of those values exceeds the precision, so we practically get 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4bf145-b0d2-4d25-8fd6-79ba1c712564",
   "metadata": {},
   "outputs": [],
   "source": [
    "lhm1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb24e6c6-6d45-4f4d-830f-e08f5b713591",
   "metadata": {},
   "source": [
    "In order to solve this we get the logs of each value and sum them up using the product rule of logarithms:\n",
    "\n",
    "${\\textstyle \\log _{b}(xy)=\\log _{b}x+\\log _{b}y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edebd6e6-cd76-477f-b62c-175fec36eb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "llh1 <- sum(log(lh1))\n",
    "llh1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5b3c6-f176-4bae-84c6-e2991a338b46",
   "metadata": {},
   "source": [
    "Since logarithm of a value in the (0, 1) interval is negative, the log-likelihood of the estimated model is also negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e1165-b456-44d1-86d4-1ec688f6bb8b",
   "metadata": {},
   "source": [
    "As we did with the SSE values in OLS, let's write a function to calculate the log-likelihood of the model with a set of parameter estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179e828-6be8-4d30-a846-3b7197dbdbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llhx <- function(b0x, b1x, datax)\n",
    "{\n",
    "    datax <- copy(datax)\n",
    "    datax[, y_fit := logistic(b0x + b1x * xterm)]\n",
    "    datax[, sum(log(ifelse(yterm, y_fit, 1 - y_fit)))]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f7193-1c9e-48f9-ab4c-7b5f1aa90e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b0s <- seq(-6, 2, 0.5)\n",
    "b1s <- seq(-1, 4, 0.5)\n",
    "param_dt <- crossing(b0 = b0s, b1 = b1s)\n",
    "setDT(param_dt)\n",
    "param_dt[, ind := .I]\n",
    "param_dt[, llh := llhx(b0, b1, data1), by = ind]\n",
    "param_dt[, maxllh := max(llh)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c85ba6f-26a5-4e99-8902-a4f24fb8d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ly() %>% \n",
    "      add_trace(data = param_dt,  x = ~b0, y = ~b1, z = ~llh, type=\"mesh3d\") %>%\n",
    "      add_trace(data = param_dt,  x = ~b0, y = ~b1, z = ~maxllh, type=\"mesh3d\") %>%\n",
    "        layout(autosize = F, width = 800, height = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f934e4e4-1275-4b30-86f0-31a6a4540a19",
   "metadata": {},
   "source": [
    "Here, since log-likelihood function is concave, we will try to estimate the parameter set that maximizes the log-likelihood such that we get the value on the blue surface where it touches the orange plane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a31490e-163c-4206-9aca-0736860dd0fd",
   "metadata": {},
   "source": [
    "Due to the mathematical properties of the log-likelihood function we cannot directly calculate its maxima, however we can follow an iterative approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a19a0e-d844-4935-9c11-1b95e2bdd05d",
   "metadata": {},
   "source": [
    "The method to be used is *iteratively reweighted least squares* (IRLS), which is equivalent to maximizing the log-likelihood of a Bernoulli distributed process using Newton's method.\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bb5608-dab8-4400-98b0-5cbb4e2c2365",
   "metadata": {},
   "source": [
    "So what is Newton-Raphson method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4abeca-f564-4010-a4ad-f54943238830",
   "metadata": {},
   "source": [
    "At an extrema point the slope of a curve becomes 0:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cf8ce4b-9dd9-4d5c-99dd-0eeb7f5dbd3f",
   "metadata": {},
   "source": [
    "![maxima](../imagesba/maxima.svg)\n",
    "\n",
    "(https://www.mathsisfun.com/calculus/maxima-minima.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f73de8-7282-46e3-ac0a-aaa01ed05223",
   "metadata": {},
   "source": [
    "So the main task is to find the root (the x value where y becomes 0) of the slope or the first derivative or the gradient of the maximum likelihood function.\n",
    "\n",
    "We can start at an arbitrary value at the curve of the first derivative:\n",
    "\n",
    "- Draw a tangent line to the derivative curve,\n",
    "- Get the point where the tangent intersects the x axis\n",
    "- And get to the y point of that x coordinate on the derivative curve\n",
    "- Repeat the above 3 steps until the y value on the curve converges to 0.\n",
    "- The final x value is the root of the derivative curve - where it becomes 0 - and also the point where the original curve reaches its extrema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42b80e9-9977-461d-b865-aa949ed88ad3",
   "metadata": {},
   "source": [
    "![newton-raphson](../imagesba/newton_raphson.gif)\n",
    "\n",
    "(https://medium.com/@ruhayel/an-intuitive-and-physical-approach-to-newtons-method-86a0bd812ec3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d7be6b-91df-4eda-8cda-decf761f2436",
   "metadata": {},
   "source": [
    "The general equation for the iterative method to find the root is:\n",
    "\n",
    "${\\displaystyle x_{1}=x_{0}-{\\frac {f(x_{0})}{f'(x_{0})}}}$\n",
    "\n",
    "Where the next iteration of the x value adds the ratio of the y value divided by the slope of the y value to the previous x value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39f1c97-7288-4c20-8577-4e2582e6b361",
   "metadata": {},
   "source": [
    "$\\displaystyle \\beta^{t+1} = \\frac {\\nabla_{\\beta}l(\\beta^t)}{\\nabla_{\\beta\\beta}l(\\beta^t)}$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\beta^{t+1}$ is the value of parameters in the next iteration\n",
    "- $\\beta$ is the value of parameters in the last iteration\n",
    "- $\\nabla_{\\beta}l(\\beta^t)$ is the gradient (first order derivative) of the log-likelihood function w.r.t. $\\beta_t$ also known as the *score function*\n",
    "- $\\nabla_{\\beta}l(\\beta^t)$ is the Hessian (second order derivative) matrix of the log-likelihood function w.r.t. $\\beta_t$, negative of which is also known as the *observed information matrix*\n",
    "\n",
    "(https://arunaddagatla.medium.com/maximum-likelihood-estimation-in-logistic-regression-f86ff1627b67)\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Generalized_linear_model#Maximum_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9414c2d-b7c1-4906-80e2-c593773a4d52",
   "metadata": {},
   "source": [
    "So after the initial guess of parameter values at the first iteration, each next iteration is calculated by:\n",
    "\n",
    "$\\displaystyle \\hat \\beta_{t+1} = \\hat \\beta_{t} + (X^TW_{t}X)^{-1}X^T(y - \\hat y_{t})$\n",
    "\n",
    "where:\n",
    "\n",
    "- $X$ is the matrix of predictor values and $X^T$ its transpose\n",
    "- $y$ is the vector of actual response values\n",
    "- $\\hat y_{t}$ is the vector of predicted values which are basically ${\\displaystyle \\mathbf {\\hat y_t} =\\sigma(\\mathbf {X} {\\boldsymbol {\\hat \\beta_t }}})$ and $\\sigma$ is the logistic function.\n",
    "- $W_{t}$ is the diagonal matrix in which the entries are the $\\hat y_t(1 - \\hat y_t)$ values.\n",
    "\n",
    "(https://www.statlect.com/fundamentals-of-statistics/logistic-model-maximum-likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98dec9f-f854-4ef0-9055-de0afa1a2c95",
   "metadata": {},
   "source": [
    "Let's implement this *iterated reweighted least squares* method in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a3b9ca-4c92-4415-ad65-58e154f7b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "irls <- function(bt, y, x)\n",
    "{\n",
    "    yt <- logistic(x %*% bt)\n",
    "    wt <- diag(as.vector(yt * (1 - yt)))\n",
    "    bt + solve(t(x) %*% wt %*% x) %*% t(x) %*% (y - yt)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af717a2-c401-4908-91cb-80f97102832b",
   "metadata": {},
   "source": [
    "Combine the initial parameters into a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b9003-9c0d-4238-8b62-7abe68209a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "binit <- as.matrix(c(b00, b10))\n",
    "binit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d673a883-4bb6-4445-ad49-aac2cc19cfe9",
   "metadata": {},
   "source": [
    "And create matrices for x and y terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93030722-2d69-4ad8-95c9-e8a958f627b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yx <- as.matrix(data1$yterm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ae017-f8fb-4e26-9bf1-693bb9169bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmat <- cbind(1, data1$xterm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007eea13-de95-47e7-a15d-62b4749c2016",
   "metadata": {},
   "source": [
    "Let's create an empty list to collect parameter values in each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191e5f6-e9c1-4e76-87a1-3866375317f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_l <- list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c887c70a-68f0-424b-a9b5-07b553256fc3",
   "metadata": {},
   "source": [
    "Let's set a precision value so that the algorithm stops when difference between the parameter values in each subsequent iteration is below that precision and hence our algorithm converged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f37de-b321-4d89-ac0c-c1eadaaf27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec <- 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db12b08-faee-48e4-8222-9b52a4b1ae18",
   "metadata": {},
   "source": [
    "Let's initiate the iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2d347-76bc-4a78-85df-ff47d896f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter <- 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f2f6a-c600-4272-91b8-fc7ecbc6dd0b",
   "metadata": {},
   "source": [
    "Assign the initial parameter values into the parameter value of the iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11514fa-e98b-4e07-8a8c-b8215fb9271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "biter <- binit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21989a77-6dcd-4c76-8d3a-c37bff778f65",
   "metadata": {},
   "source": [
    "Initiate a matrix of arbitrarily large parameter values so that in the initial iteration we ensure that we haven't converged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b9cf9-2376-469c-98fa-a3dbfe9fbb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprev <- as.matrix(c(Inf, Inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00338c5f-0ffd-47d5-bb77-c97be6b24530",
   "metadata": {},
   "source": [
    "Let's save the initial parameters as the first item of the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c492c8d5-4cb8-42e8-b32f-732483cc17cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_l[[iter]] <- biter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080d263-5a0c-4b84-a4cc-887c3ac3f8b5",
   "metadata": {},
   "source": [
    "Let's run the iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918f9811-4438-4bed-8586-cf044230b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "while (sum(abs(biter - bprev)) > prec) # as long as not converged\n",
    "{\n",
    "    iter <- iter + 1 # increment iterations\n",
    "    bprev <- biter # save the last parameters\n",
    "    biter <- irls(bprev, yx, xmat) # calculate next parameters\n",
    "    param_l[[iter]] <- biter # save the next parameters in the list\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99f63a8-c37f-46cb-9d51-e5c9520ec7c7",
   "metadata": {},
   "source": [
    "We have done only eight iterations for the convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9962ef-7883-4d7b-9bd7-0252adbf9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf9f2cd-7215-4fcb-9815-2df580d98200",
   "metadata": {},
   "source": [
    "We see that we have converged sufficiently to the true parameter values of -4 and 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879c6293-c35f-4ead-b652-f1d9bf6ac4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t(do.call(cbind, param_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab83a4-9e41-426e-ae79-eca37786199b",
   "metadata": {},
   "source": [
    "Now for each iteration, let's calculate the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f07e43e-3c3f-4ce4-a900-2acae4d0c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_l <- lapply(param_l, function(x) logistic(xmat %*% x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89891e6-1f42-446d-9302-8f5f7157671b",
   "metadata": {},
   "source": [
    "And calculate the log-likelihood of the estimated model in each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c6032e-167b-4290-8eb3-3eb98aa62998",
   "metadata": {},
   "outputs": [],
   "source": [
    "llh_l <- sapply(pred_l, function(x) sum(log(ifelse(data1$yterm, x, 1 - x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c4efc-d1ad-44d1-910f-63e694efb224",
   "metadata": {},
   "outputs": [],
   "source": [
    "llh_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8cef3-a9d6-45d3-a7e5-bcd1965b5d5b",
   "metadata": {},
   "source": [
    "While the parameter values converged sufficiently, the log-likelihood also converged sufficiently to its max value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d451df62-10c5-427e-b1ea-a38fffeed11a",
   "metadata": {},
   "source": [
    "Now let's combine actual y values, predicted y values - the probabilities - and the iteration counts into a single table. Note that in order to make the interactive plots lighter, only a portion of the data points will be filtered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39adc252-eaf7-485d-91fd-a1ced7d3f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dt <- mapply(function(a, x, y, z) data.table(xterm = a[1:5e2], ypred = as.vector(x)[1:5e2], yterm = y[1:5e2], iter = z), list(data1$xterm), pred_l, list(data1$yterm), seq_along(pred_l), SIMPLIFY = F) %>% rbindlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289d65b-ba88-4ebd-bfc9-85675f7cce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(pred_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd7546-d002-44fe-b8a1-3ac79f4c674d",
   "metadata": {},
   "source": [
    "Let's see how the model predictions, as shown by the S curves converge to the true model in each iteration. A random jitter is added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada8fe6-678e-4276-b211-436cf525b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(30)\n",
    "p1 <- pred_dt %>%\n",
    "mutate_at(\"iter\", as.factor) %>%\n",
    "filter(iter == 1) %>%\n",
    "ggplot(aes(x = xterm, y = yterm)) +\n",
    "geom_jitter(height = 0.1) +\n",
    "geom_line(data = pred_dt %>% mutate_at(\"iter\", as.factor),\n",
    "          aes(x = xterm, y = ypred, color = iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c355b01a-6fe4-4bae-8cd2-2e0b09d9d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (T) ggplotly(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc491a6-89d9-4c85-b415-82e20ebcfa7b",
   "metadata": {},
   "source": [
    "Or animate the iterations with random jitters again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcbcad0-8bee-4560-888f-23e3a9ffcfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(40)\n",
    "if (T)\n",
    "{\n",
    "    pred_dt %>%\n",
    "    mutate(ytermjit = jitter(yterm, amount = 0.15)) %>%\n",
    "    arrange(iter, xterm) %>%\n",
    "    plot_ly(x = ~xterm) %>%\n",
    "    add_trace(y = ~ytermjit, type = \"scatter\", mode = \"markers\") %>%\n",
    "    add_trace(y = ~ypred, frame = ~iter, type = 'scatter', mode = \"lines\") %>%\n",
    "    animation_opts(\n",
    "        frame = 500, redraw = T, easing = \"linear\", mode = \"next\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f442975-ea06-4b77-a651-904287e22a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf576cb-8bc8-415e-9dc4-90b6f380fd4c",
   "metadata": {},
   "source": [
    "## glm() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062aff7-0dfd-4574-b612-ab9c71dedca5",
   "metadata": {},
   "source": [
    "We don't have to go through these iterative steps manually, and we will use the built in `glm()` function for generalized linear models and for logistic regression we pass the *binomial* for the model family:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d270f-1548-46ce-b48b-b06168fa61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glm <- glm(yterm ~ xterm, data1, family = \"binomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ad0e2-af6b-416f-a651-54a14250afed",
   "metadata": {},
   "source": [
    "Let's see the model results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26726e8e-70e3-4f2c-ba52-7bdbb9e27a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glm_sum <- summary(model_glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c8e81a-2b7c-4dcc-bd55-daa7d85b771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glm_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98714da-e988-409a-aa21-7a84dfd59ecc",
   "metadata": {},
   "source": [
    "### Deviances and Likelihood Ratio Test (LRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738f919-c884-4a9f-b69a-4ab13d38c3ba",
   "metadata": {},
   "source": [
    "First of all let's check whether our model is doing significantly better than the null model - with only intercept - given the number of parameters added.\n",
    "\n",
    "First let's calculate the *null deviance* value which is -2 times the log-likelihood of the null model. So we will just calculate the mean of the response variable as the probability of success and calculate the log-likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1869ea9b-42a0-4363-836e-72dd65e80f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "nulld1 <- -2 * sum(log(ifelse(data1$yterm == 1, mean(data1$yterm), 1 - mean(data1$yterm))))\n",
    "nulld1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e8d4b-4ab7-487b-853f-3a2ba9d58310",
   "metadata": {},
   "source": [
    "And compare with the null deviance from model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4340bc9-53c7-4df8-af9d-5fa881ebb076",
   "metadata": {},
   "outputs": [],
   "source": [
    "nulld2 <- model_glm_sum$null.deviance\n",
    "nulld2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cba9a09-0ea9-4995-93df-187591d34310",
   "metadata": {},
   "source": [
    "They are the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745eeaf9-061b-4804-ad58-5be0ba41d25b",
   "metadata": {},
   "source": [
    "The degrees of freedom is $n - 1$ since we only calculate the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185e58c-afe5-49e7-b247-34c247989717",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[, .N] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d235a-d99c-4cb1-b498-928a80530987",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnull <- model_glm_sum$df.null\n",
    "dfnull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d563235b-b31e-4df9-9a72-e531a32bcec1",
   "metadata": {},
   "source": [
    "And now, let's calculate the residual deviance, -2 times the log-likelihood of the estimated model. So we extract the fitted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f181cb6e-2021-496d-9ff0-2ae26f3addf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "residd1 <- -2 * sum(log(ifelse(data1$yterm == 1, model_glm$fitted, 1 - model_glm$fitted)))\n",
    "residd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e752918c-0452-4014-bc5d-66d0364c1cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "residd2 <- model_glm_sum$deviance\n",
    "residd2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a9233-5588-47a7-bef0-6f77e391f71b",
   "metadata": {},
   "source": [
    "Again the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6ebd5-9e6f-4308-b325-421a2a457eb0",
   "metadata": {},
   "source": [
    "df is n - 2 since we estimate two parameters b0 and b1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ef2af-32ed-4c12-b1fa-05e0df386f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[, .N] - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45626d-1e88-447c-82a8-c6e592eb7fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmodel <- model_glm_sum$df.residual\n",
    "dfmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdeb0dc-25c9-4985-8495-7b62b371a032",
   "metadata": {},
   "source": [
    "Now we will conduct a likelihood ratio test, where the test statistic $\\chi^2$ distributed and is the difference in deviance values and degrees of freedom is the difference in df values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0407b71-bec5-4f56-a404-6e3d6dea020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pchisq(nulld1 - residd1, dfnull - dfmodel, lower.tail = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653f4cad-95fb-46ac-b8a9-ea99bd5b0485",
   "metadata": {},
   "source": [
    "We can also do the same using `anova` function. Note that simpler model is compared to the complex model, so the first parameter is passed as the null model or intercept only model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d11065f-b326-4f93-b3c2-ae747d0337f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "anova(update(model_glm, . ~ 1), model_glm, test = \"LRT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229e9715-809b-4c40-835f-b9dbd12b9c44",
   "metadata": {},
   "source": [
    "So our model does better than the null model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa6f80-ff3e-4462-8625-4ec1c34a5fac",
   "metadata": {},
   "source": [
    "### Akaike Information Criterion (AIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b085c-ba87-4cd1-aed1-cd49b0d91b94",
   "metadata": {},
   "source": [
    "Akaike information criterion can be used in order to compare models with different number of parameters:\n",
    "\n",
    "${\\displaystyle \\mathrm {AIC} \\,=\\,2k-2\\ln({\\hat {L}})}$\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Akaike_information_criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e474a-0007-4a8a-84ab-1b42cba30a6d",
   "metadata": {},
   "source": [
    "Since we have two parameters in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d005ddb7-3f1d-4e27-8a99-54715a4e51b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * nrow(model_glm_sum$coefficients) + residd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead35073-76ba-4c14-8740-8ce3da0ffb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glm_sum$aic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6403e9f-7259-4873-a74d-00933076a0fd",
   "metadata": {},
   "source": [
    "On a stand alone basis AIC has not much to do. And in comparing different models, likelihood ratio test can also be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd321bb-5826-4552-81fa-05588fa9e91a",
   "metadata": {},
   "source": [
    "### Significance of Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3551514-4bd1-4cbc-9ed6-0e1972e532dc",
   "metadata": {},
   "source": [
    "Now let's come to the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b524d-1b47-4fa5-986c-db5944f10557",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glm_coef <- tidy(model_glm)\n",
    "setDT(model_glm_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6dd44-0a34-4cf2-9344-3bfe79548d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glm_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8322bbd-bd22-4a60-85e5-91d1c781709c",
   "metadata": {},
   "source": [
    "To get the standard errors, the inverse of the observed information matrix - negative of the Hessian matrix or second derivative of log-likelihood w.r.t $\\beta$ - is calculated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e3192b-2d38-4fd1-8da7-1a49c7b189d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve(t(xmat) %*% diag(model_glm$fitted * (1 - model_glm$fitted)) %*% xmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f43e4d-dd2a-4788-8973-3a5aff08dfa1",
   "metadata": {},
   "source": [
    "This is the variance covariance matrix of the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2698099-13d2-4fda-a7d0-01be52a9ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vcov(model_glm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe99dd-e7f2-4dd1-a4b2-bc99ca1e1366",
   "metadata": {},
   "source": [
    "So basically we take the diagonal of the variance-covariance matrix and get its square root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0ccc5-f1f2-4aa0-8044-8ef80fc23146",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt(diag(vcov(model_glm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8699b94a-cb8f-42fe-9b01-e48f1f2c1d07",
   "metadata": {},
   "source": [
    "These are the same values reported in the coefficients table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e24f4-360c-4d10-b62a-b0a7ae345324",
   "metadata": {},
   "source": [
    "And test statistics are calculated by dividing the coefficients with their standard errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2352fd-8805-47e4-a058-523e81bcd2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glm_coef[, estimate / std.error]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400366be-20b2-4b12-9d7d-1bd7a7ee2668",
   "metadata": {},
   "source": [
    "To assess their significance a Wald test is conducted and for logistic model, the estimated parameter values are assumed to follow a normal distribution in maximum likelihood estimation. So basically we conduct a z-test using normal distribution instead of t-test using Student's t-distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b4e1e-d0f8-45f5-bb01-df3dfb33ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * pmin(pnorm(model_glm_coef$statistic), 1 - pnorm(model_glm_coef$statistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a6792c-16f3-4003-9f55-09d25707621b",
   "metadata": {},
   "source": [
    "Basically we have a highly significant p-value, practically zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb2fbea-94ba-4130-b9fd-72ea86aab451",
   "metadata": {},
   "source": [
    "### Interpretation of Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b7b1c-3890-497b-a600-d2a4c60724ef",
   "metadata": {},
   "source": [
    "Contrary to multiple linear regression, in which the effect of coefficients on the response variable can be interpreted easily in their original scales, the interepretation of the coefficients in a logistic regression model is trickier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d4dea-c12a-4d10-904c-257da91a470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glm_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8a2a7-946e-4cfd-a659-78147ec6fc14",
   "metadata": {},
   "source": [
    "Since the link function is logit (and inverse link function is logistic), the output of the linear combination is interpreted as log odd values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f37d3e8-3acd-4ff5-87f0-eac2084016b4",
   "metadata": {},
   "source": [
    "In terms of log-odds we can say that:\n",
    "\n",
    "- The intercept term is -3.95, hence when predictor values are set to zero, the log-odds takes a value of -3.95\n",
    "- For every 1 increase in xterm, the log-odds increase by 1.96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7d5f63-13bd-4506-8d3f-ed8f286a2f03",
   "metadata": {},
   "source": [
    "We can also interpret in odd values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c658e73a-9d6c-4d3c-a563-a8526568556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp(model_glm_coef$estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d835e08-b766-48e8-a8b9-ed719aa0f689",
   "metadata": {},
   "source": [
    "- When predictor values are set to zero, the odds takes a value of 0.019\n",
    "- For every 1 increase in xterm, the odds increase by 6.096 (7.096 - 1)\n",
    "\n",
    "Since relationship between the coefficients and the probability values are not linear, coefficients' effect on predicted probabilities cannot be interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43021333-a7d6-4f6e-ba14-dd4b7cb745c3",
   "metadata": {},
   "source": [
    "# Classification Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3a982-068c-4a35-8526-24bb3a4d9805",
   "metadata": {},
   "source": [
    "In the synthetic data we created, the response variable can take only one of the two values: 0 and 1. So our model is basically trying to predict one of these two values for each observation, a task known as *classification*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51bbdd4-a5c0-4ace-8d39-73a3aa282668",
   "metadata": {},
   "source": [
    "The performance of a classification model can be assessed on how well the classes are predicted.\n",
    "\n",
    "Two basic methods are conducted to assess the classification performance:\n",
    "\n",
    "- Confusion matrix\n",
    "- Receiver Operating Characteristic Curve (ROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb500d-b49a-446c-a975-a2ab21c4a191",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d322a27a-56cd-4e24-8d3d-813bb124fac9",
   "metadata": {},
   "source": [
    "In order to understand a confusion matrix, let's first define a positive and a negative class:\n",
    "\n",
    "- A *negative* class shows the absence of an effect, so basically we can liken it to a null hypothesis. Negative case is usually the majority of the observations. For example in the context of a medical test, negative cases are the ones where a condition is not found, in the case of spam detection a negative case is when the mail is NOT spam.\n",
    "\n",
    "- A *positive* class shows an effect and usually comprise a minority of the total observations. That is the class of interest The observations where a medical condition is found, a spam mail or a defaulted loan are examples of positive classes.\n",
    "\n",
    "\n",
    "We can either make a true of false prediction of the cases:\n",
    "- A *true* prediction is made when the actual and predicted classes are the same\n",
    "- A *false* prediction is made when the actual and predicted classes are different"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7082ad0-8805-4343-a7b4-4589c80b16d9",
   "metadata": {},
   "source": [
    "A confusion matrix of a variable of two classes have four parts:\n",
    "\n",
    "\n",
    "<table class=\"wikitable\" style=\"border:none; background:inherit;color:inherit; text-align:center;\" align=\"center\">\n",
    "<tbody><tr>\n",
    "<td rowspan=\"2\" style=\"border:none;\">\n",
    "</td>\n",
    "<td style=\"border:none;\">\n",
    "</td>\n",
    "<td colspan=\"2\" style=\"background:#4ad2d260;color:inherit;\"><b>Predicted condition</b>\n",
    "</td></tr>\n",
    "<tr>\n",
    "<td style=\"background:#c9c9c950;color:inherit;\"><a href=\"/wiki/Statistical_population\" title=\"Statistical population\">Total population</a> <br><span style=\"white-space:nowrap;\">= P + N</span>\n",
    "</td>\n",
    "<td style=\"background:#78ffff60;color:inherit;\"><b>Positive (PP)</b>\n",
    "</td>\n",
    "<td style=\"background:#1da5a560;color:inherit;\"><b>Negative (PN)</b>\n",
    "</td></tr>\n",
    "<tr>\n",
    "<td rowspan=\"2\" class=\"nowrap ts-vertical-header is-valign-middle\" style=\"background:#d2d23d60;color:inherit;\"><div style=\"\"><style data-mw-deduplicate=\"TemplateStyles:r1221560606\">@supports(writing-mode:vertical-rl){.mw-parser-output .ts-vertical-header{line-height:1;max-width:1em;padding:0.4em;vertical-align:bottom;width:1em}html.client-js .mw-parser-output .sortable:not(.jquery-tablesorter) .ts-vertical-header:not(.unsortable),html.client-js .mw-parser-output .ts-vertical-header.headerSort{background-position:50%.4em;padding-right:0.4em;padding-top:21px}.mw-parser-output .ts-vertical-header.is-valign-top{vertical-align:top}.mw-parser-output .ts-vertical-header.is-valign-middle{vertical-align:middle}.mw-parser-output .ts-vertical-header.is-normal{font-weight:normal}.mw-parser-output .ts-vertical-header>*{display:inline-block;transform:rotate(180deg);writing-mode:vertical-rl}@supports(writing-mode:sideways-lr){.mw-parser-output .ts-vertical-header>*{transform:none;writing-mode:sideways-lr}}}</style><b>Actual condition</b></div>\n",
    "</td>\n",
    "<td style=\"background:#ffff7860;color:inherit;\"><b>Positive (P)</b>\n",
    "</td>\n",
    "<td style=\"background:#78ff7860;color:inherit;\"><b><a href=\"/wiki/True_positive\" class=\"mw-redirect\" title=\"True positive\">True positive</a> (TP) <br></b>\n",
    "</td>\n",
    "<td style=\"background:#ffa5a560;color:inherit;\"><b><a href=\"/wiki/False_negative\" class=\"mw-redirect\" title=\"False negative\">False negative</a> (FN) <br></b>\n",
    "</td></tr>\n",
    "<tr>\n",
    "<td style=\"background:#a5a51d60;color:inherit;\"><b>Negative (N)</b>\n",
    "</td>\n",
    "<td style=\"background:#ff787860;color:inherit;\"><b><a href=\"/wiki/False_positive\" class=\"mw-redirect\" title=\"False positive\">False positive</a> (FP) <br></b>\n",
    "</td>\n",
    "<td style=\"background:#3dd23d60;color:inherit;\"><b><a href=\"/wiki/True_negative\" class=\"mw-redirect\" title=\"True negative\">True negative</a> (TN) <br></b>\n",
    "</td></tr>\n",
    "<tr>\n",
    "<td colspan=\"4\" style=\"border:none;\"><sup>Sources: </sup><sup id=\"cite_ref-4\" class=\"reference\"><a href=\"#cite_note-4\"><span class=\"cite-bracket\">[</span>4<span class=\"cite-bracket\">]</span></a></sup><sup id=\"cite_ref-5\" class=\"reference\"><a href=\"#cite_note-5\"><span class=\"cite-bracket\">[</span>5<span class=\"cite-bracket\">]</span></a></sup><sup id=\"cite_ref-Powers2011_2-1\" class=\"reference\"><a href=\"#cite_note-Powers2011-2\"><span class=\"cite-bracket\">[</span>2<span class=\"cite-bracket\">]</span></a></sup><sup id=\"cite_ref-6\" class=\"reference\"><a href=\"#cite_note-6\"><span class=\"cite-bracket\">[</span>6<span class=\"cite-bracket\">]</span></a></sup><sup id=\"cite_ref-7\" class=\"reference\"><a href=\"#cite_note-7\"><span class=\"cite-bracket\">[</span>7<span class=\"cite-bracket\">]</span></a></sup><sup id=\"cite_ref-8\" class=\"reference\"><a href=\"#cite_note-8\"><span class=\"cite-bracket\">[</span>8<span class=\"cite-bracket\">]</span></a></sup><sup id=\"cite_ref-9\" class=\"reference\"><a href=\"#cite_note-9\"><span class=\"cite-bracket\">[</span>9<span class=\"cite-bracket\">]</span></a></sup>\n",
    "</td></tr></tbody></table>\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12544983-44f5-4545-8989-90bbe7a70ac0",
   "metadata": {},
   "source": [
    "The meaning of these parts are:\n",
    "\n",
    "- True Positive (TP): Correctly classified as the class of interest\n",
    "- True Negative (TN): Correctly classified as not the class of interest\n",
    "- False Positive (FP): Incorrectly classified as the class of interest\n",
    "- False Negative (FN): Incorrectly classified as not the class of interest\n",
    "\n",
    "(Lantz 2015, Machine Learning with R, Ch 10, p.318)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a4427-d846-4c23-a129-bbe48b066481",
   "metadata": {},
   "source": [
    "Now let's calculate the confusion matrix for our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562dc866-a868-4608-8557-c274139799a4",
   "metadata": {},
   "source": [
    "We should first get the actual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca8826-81d6-4f9e-91ab-2edc6faec20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual1 <- data1$yterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463dfd24-e3cc-42dc-9195-de2fa7a767c9",
   "metadata": {},
   "source": [
    "And then fitted values can be extract by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e0310-d636-4301-b3f0-349e136b1327",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit1 <- model_glm$fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becae59d-6e93-441f-ad30-227ee87a42e4",
   "metadata": {},
   "source": [
    "Or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a48a82-bcf6-4219-a4bf-e268d6b90cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit2 <- predict(model_glm, type = \"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad53f99-a25e-437c-8c4d-ffd61a44f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "identical(fit1, fit2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f48926-7770-4833-a372-d2990cc3b59d",
   "metadata": {},
   "source": [
    "The second method can also be used to get predictions for a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169ab528-0603-41d1-bd58-75ecf0f68b17",
   "metadata": {},
   "source": [
    "Note that when the *response* value is not passed to *type* parameter, the linear predictions are returned, which should be converted into probabilities with the `logistic` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13646cf-dfbf-419f-9746-bae6a8f1ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(fit2, logistic(predict(model_glm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c62cefb-ef02-4548-ae41-0778bd8c2238",
   "metadata": {},
   "source": [
    "However these values are continuous between 0 and 1. In order to transform them into fitted classes, a cut point should be taken. The usual cut point is 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceeb4ea-484a-42ef-8aab-419f1ce7116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_class <- ifelse(fit1 > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7486f6-a86d-48e8-8a6b-a2bf154815fb",
   "metadata": {},
   "source": [
    "To get the confusion matrix correctly we first get the contingency table of the fitted and actual values (in this order) and pass it to `confusionMatrix` function from the `caret` package. The positive class is defined as \"1\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6e6381-dbb7-4d11-abca-583c72fd2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat <- table(fitted = fit_class, actual = actual1) %>% confusionMatrix(positive = \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b1a2f-bb2a-4288-97e9-1dc1863420e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6231753d-04d9-4480-bb0a-54bf49dbe184",
   "metadata": {},
   "source": [
    "We can extract metrics from the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf3ad46-427c-4d2e-b48b-6aa90672f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat$overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463f693-8d30-4370-b6e0-1d60419a51ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat$byClass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe57b79-d9e8-4370-a5df-6d1c6acfd2cf",
   "metadata": {},
   "source": [
    "In order to replicate the calculations and interpret their meanings, first assign the four values into their names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ebc49-51fe-4836-bec9-7a6e37738c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "conttab <- table(fitted = fit_class, actual = actual1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eae4e1-9b73-4edd-ab33-8fa01ae3dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP <- conttab[\"1\", \"1\"]\n",
    "FP <- conttab[\"1\", \"0\"]\n",
    "TN <- conttab[\"0\", \"0\"]\n",
    "FN <- conttab[\"0\", \"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810486e4-11fa-4748-a9eb-407c7561feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP\n",
    "FP\n",
    "TN\n",
    "FN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5bb2a7-d14f-41b9-92e5-6a72250bf91e",
   "metadata": {},
   "source": [
    "Note that actual negative values are classified either as:\n",
    "\n",
    "- True Negatives (TN) or\n",
    "- False Positives (FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc295fdb-8565-49dd-b32e-8234718868a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "AN <- TN + FP\n",
    "AN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99497869-0b99-437d-8d84-42ed2ffe7d96",
   "metadata": {},
   "source": [
    "And actual positive values are classified either as:\n",
    "\n",
    "- True Positives (TP) or\n",
    "- False Negatives (FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9c09a0-c6db-43a2-a5b1-a05ce4e30bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AP <- TP + FN\n",
    "AP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89721912-022d-4c34-85a6-948ab8e789b9",
   "metadata": {},
   "source": [
    "The accuracy is the ratio of correctly predicted classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f7809b-1b78-40e6-a10d-1704a0662b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "(TP + TN) / (AP + AN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4085ae8-e5ce-46cc-8eeb-6f3e85471ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accx <- confmat$overall[\"Accuracy\"]\n",
    "accx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8c730-9cea-4607-96bf-fa55b2158f4b",
   "metadata": {},
   "source": [
    "We could have chosen the majority class and predicted all values as that majority class value, which sets the no-information rate or the null value of accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c80b7-1db3-422b-a15b-f0ec01ee8ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(AP, AN) / (AP + AN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816a219-9627-4194-8616-4ac8b330e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "accn <- confmat$overall[\"AccuracyNull\"]\n",
    "accn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba2f4a7-4f72-4a46-8792-47ed4664d7a2",
   "metadata": {},
   "source": [
    "The 95% confidence interval of the accuracy rate is calculated using the binomial distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6af31-8a4d-4789-9547-c75dd09a1d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "qbinom(0.025, 10000, 0.7744) / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af55459f-6143-4b12-abf9-cda59a0866af",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat$overall[\"AccuracyLower\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a73132-7699-4151-acfa-16fedf28e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qbinom(1-0.025, 10000, 0.7744) / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65494b57-7e7c-4222-96da-ffe5e3ed24f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat$overall[\"AccuracyUpper\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb13ed-265f-4008-84b9-8caed927ba7f",
   "metadata": {},
   "source": [
    "We see that, the null accuracy value or no information rate is outside this interval. So we did significantly better than selecting the majority class. That can be confirmed with the p-value also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c624b-d03b-4775-a199-30a5a4f539f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbinom(0.7744, 10000, 0.5047)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae41ecb6-9237-4781-b735-f3377f0eae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat$overall[\"AccuracyPValue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dfc9ac-e048-43d6-8097-77deb6e9b436",
   "metadata": {},
   "source": [
    "The kappa statistic (labeled Kappa in the previous output) adjusts accuracy by accounting for the possibility of a correct prediction by chance alone.\n",
    "\n",
    "This is especially important for datasets with a severe class imbalance, because a classifier can obtain high accuracy simply by always guessing the most frequent class.\n",
    "\n",
    "The kappa statistic will only reward the classifier if it is correct more often than this simplistic strategy.\n",
    "\n",
    "Kappa values range from 0 to a maximum of 1, which indicates perfect agreement between the model's predictions and the true values. Values less than one indicate imperfect agreement. Depending on how a model is to be used, the interpretation of the kappa statistic might vary. One common interpretation is shown as follows:\n",
    "- Poor agreement = less than 0.20\n",
    "- Fair agreement = 0.20 to 0.40\n",
    "- Moderate agreement = 0.40 to 0.60\n",
    "- Good agreement = 0.60 to 0.80\n",
    "- Very good agreement = 0.80 to 1.00\n",
    "\n",
    "(Lantz 2015, Machine Learning with R, Ch 10, p.323)\n",
    "\n",
    "The formula is:\n",
    "\n",
    "${\\displaystyle \\kappa ={\\frac {2\\times (TP\\times TN-FN\\times FP)}{(TP+FP)\\times (FP+TN)+(TP+FN)\\times (FN+TN)}}}$\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Cohen%27s_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3112504-68c6-4843-9c2a-f103632848f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat$overall[\"Kappa\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5a1285-0d0d-4634-843f-9f04ea4621bd",
   "metadata": {},
   "source": [
    "So we have a moderate agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e2161-c261-4e58-ae2b-153fd9c198eb",
   "metadata": {},
   "source": [
    "The **sensitivity** of a model (also called the true positive rate) measures the proportion of actual positive examples that were correctly classified. Therefore, it is calculated as the number of true positives divided by the total number of positives, both correctly classified (the true positives) as well as incorrectly classified (the false negatives). **Recall** measure is calculated the same way, although the interpretation of the metric is different: A model with a high recall captures a large portion of the positive examples, meaning that it has wide breadth. For example, a search engine with a high recall returns a large number of documents pertinent to the search query. Similarly, the SMS spam filter has a high recall if the majority of spam messages are correctly identified.\n",
    "\n",
    "$\\displaystyle \\text {Sensitivity} = \\frac {TP}{TP + FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd125aa3-0a7c-4aad-8089-cb2eaad4a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff22c9-307b-4f99-8f42-a1b92e8a8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat$byClass[\"Sensitivity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e345faa-5ff8-4450-b849-8f25d724d628",
   "metadata": {},
   "source": [
    "So our model correctly classified 76.7% of actual positive cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c265907-0faa-4ec8-b2e5-4076bb1d742a",
   "metadata": {},
   "source": [
    "The **specificity** of a model (also called the true negative rate) measures the proportion of actual negative examples that were correctly classified. As with sensitivity, this is computed as the number of true negatives, divided by the total number of negatives—the true negatives plus the false positives:\n",
    "\n",
    "$\\displaystyle \\text {Specificity} = \\frac {TN}{TN + FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980a10b-becd-45b3-8bea-1fd7d2853c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "TN / (TN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc75c1ca-8d53-4766-a20c-576211d9bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat$byClass[\"Specificity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a74edcf-2d82-46d9-a469-f902f4c98fbf",
   "metadata": {},
   "source": [
    "So our model correctly classified 78.2% of actual negative cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94706c67-3545-4a4a-8be2-0b6b5c1e0b7b",
   "metadata": {},
   "source": [
    "**Precision** or **Positive predictive value** is defined as the proportion of positive examples that are truly positive; in other words, when a model predicts the positive class, how often is it correct? A precise model will only predict the positive class in cases that are very likely to be positive. It will be very trustworthy.\n",
    "\n",
    "$\\displaystyle \\text {Precision} = \\frac {TP}{TP + FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fabb21e-ad84-42a0-908c-5f8fad1decbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eb98b0-ab63-4a54-b037-2c59a9a71769",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat$byClass[\"Precision\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e00e6f-24c8-4dda-9803-193403e83800",
   "metadata": {},
   "source": [
    "So in our model, 77.5% of our positive preditions are true positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67504e9c-03ac-48d1-92ce-068e06fe60b7",
   "metadata": {},
   "source": [
    "**Negative predictive value** is defined as the proportion of negative examples that are truly negative; in other words, when a model predicts the negative class, how often is it correct?\n",
    "\n",
    "$\\displaystyle \\text {Negative predictive value} = \\frac {TN}{TN + FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624999b6-e493-4e5e-bd66-ae23adb75aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TN / (TN + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55481b9b-fd91-494c-8c88-28772c0e908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat$byClass[\"Neg Pred Value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830f8c0-2bf9-4925-8e85-803a8e3dc326",
   "metadata": {},
   "source": [
    "So in our model, 77.3% of our negative preditions are true negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c932e-ce2e-4d6a-b270-f86977d2e7c7",
   "metadata": {},
   "source": [
    "Another important metric is the *lift* value: How much are we better at identifying positive classes than random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b486cd-e89d-477d-a663-a365df5b697d",
   "metadata": {},
   "source": [
    "Baseline precision is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb1d01-38a1-4c89-bec9-1fbb2abb238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprec <- AP / (AP + AN)\n",
    "bprec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6d633-ae36-43dd-966b-9a2f251fb6ae",
   "metadata": {},
   "source": [
    "Precision or positive predictive rate of our model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71600f3-681c-4706-9b86-fc6e47f5f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat$byClass[\"Precision\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2558c7c-7cb4-46c3-ada7-bb7620d429b8",
   "metadata": {},
   "source": [
    "Lift is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8569dc9-8488-4e03-b8bc-c6fb0754922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat$byClass[\"Precision\"] / bprec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c70a7e4-fb42-4e6c-ae60-e437989826a9",
   "metadata": {},
   "source": [
    "So our model is 1.56 times better at identifying positive classes than random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a0214-841f-4b00-a76c-f5c6ee506ab0",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic Curve (ROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14701e-af69-43a5-8e6d-30888ef808fc",
   "metadata": {},
   "source": [
    "While the confustion matrix onlu considers classes, a ROC curve takes into account the predicted (or fitted) probabilities (of being in the positive class).\n",
    "\n",
    "The curve is constructed such that:\n",
    "\n",
    "- The observations are sorted from the smallest predicted probability to the largest\n",
    "- For each negative prediction the curve steps up\n",
    "- For each positive prediction the curve steps right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f18803-fdc0-4fc5-9f08-94a077037374",
   "metadata": {},
   "source": [
    "On the y axis is the sensitivity while on the x axis false positive rate (FPR) is plotted. Note that false positive rate is 1 - specificity or:\n",
    "\n",
    "$\\text {FPR} = \\frac{FP}{FP + TN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af17555-0e57-47dc-9e59-a44bbe50e683",
   "metadata": {},
   "source": [
    "![ROC](../imagesba/roc.png)\n",
    "\n",
    "Four ROC curves with different values of the area under the ROC curve (interpreted for a medical study):\n",
    "- A perfect test (A) has an area under the ROC curve of 1.\n",
    "- The chance diagonal (D, the line segment from 0, 0 to 1, 1) has an area under the ROC curve of 0.5.\n",
    "- ROC curves of tests with some ability to distinguish between those subjects with and those without a disease (B, C) lie between these two extremes.\n",
    "- Test B with the higher area under the ROC curve has a better overall diagnostic performance than test C.\n",
    "\n",
    "(https://www.researchgate.net/figure/Four-ROC-curves-with-different-values-of-the-area-under-the-ROC-curve-A-perfect-test-A_fig2_8636163)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6050dd72-1cbf-4975-b224-4f884ea84d08",
   "metadata": {},
   "source": [
    "To draw the ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1088c189-e946-487a-8dc6-8a7eddaabbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.roc(actual1, fit1, legacy.axes = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0df69a-a909-46e6-81bb-77ffddaf31c7",
   "metadata": {},
   "source": [
    "We see that classification performance can be considered high, while not very close to perfect: It is in between the perfect performance and the totally random performance case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26867a19-8ac2-4d3a-bf04-a95eeb46bf87",
   "metadata": {},
   "source": [
    "The area under the curve (often referred to as simply the AUC) is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming 'positive' ranks higher than 'negative').In other words, when given one randomly selected positive instance and one randomly selected negative instance, AUC is the probability that the classifier will be able to tell which one is which.\n",
    "\n",
    "AUC varies between 0 and 1 — with an uninformative classifier yielding 0.5.\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Receiver_operating_characteristic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af702072-4a22-45fc-b7c2-d718a293b7a7",
   "metadata": {},
   "source": [
    "The interpretation of possible values are:\n",
    "\n",
    "- A: Outstanding = 0.9 to 1.0\n",
    "- B: Excellent/good = 0.8 to 0.9\n",
    "- C: Acceptable/fair = 0.7 to 0.8\n",
    "- D: Poor = 0.6 to 0.7\n",
    "- E: No discrimination = 0.5 to 0.6\n",
    "\n",
    "(Lantz 2015, Machine Learning with R, Ch 10, p.333)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cea90b-7668-49a5-94a5-dfa9edaa28e1",
   "metadata": {},
   "source": [
    "We can calculate the AUC value of the predictions of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aca312-8ae5-4766-b09d-6177e6855b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc(actual1, fit1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff9161-7594-4c4b-8234-8dbba0d122a3",
   "metadata": {},
   "source": [
    "So we can consider the AUC value as excellent/good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5b1d0-f62e-4bc8-9678-8355d17c842f",
   "metadata": {},
   "source": [
    "# Resources on GLM, Logistic Regression and Classification Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f89714-22e1-42bd-b6cb-bac5bcff6ba6",
   "metadata": {},
   "source": [
    "- Agresti and Kateri 2021, Foundations of Statistics for Data Scientists, Ch.7\n",
    "- James et al. 2023, An Introduction to Statistical Learning, Ch.4\n",
    "- Kaplan and Pruim, Statistical Modeling: A Fresh Approach, Ch.17\n",
    "- Roback and Legler 2021, Beyond Multiple Linear Regression, Ch.5,6\n",
    "- Wood 2017, Generalized Additive Models, Ch.3\n",
    "- McElreath 2020, Statistical Rethinking, Ch.10 (for Bayesian point of view)\n",
    "- Lantz 2015, Machine Learning with R, Ch.10 (for classification evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
